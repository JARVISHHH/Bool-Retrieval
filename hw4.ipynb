{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 布尔查询之BSBI与索引压缩\n",
    "\n",
    "由于BSBI与索引压缩对于索引的存储方式有着一定的要求，所以之前的框架不再适用，本次作业使用斯坦福大学[CS 276 / LING 286: Information Retrieval and Web Search](https://web.stanford.edu/class/cs276/)课程的代码框架来实现。具体来说主要包含的内容有：\n",
    "1. [索引构建 (40%)](#索引构建与检索-(40%)) 使用BSBI方法模拟在内存不足的情况下的索引构建方式，并应用于布尔查询\n",
    "2. [索引压缩 (30%)](#索引压缩-(30%)) 使用可变长编码对构建的索引进行压缩\n",
    "3. [布尔检索 (10%)](#布尔联合检索-(10%)) 对空格分隔的单词查询进行联合（与）布尔检索\n",
    "3. [实验报告 (10%)](#Report-(25%)) 描述你的代码并回答一些问题\n",
    "4. [额外的编码方式 (10%)](#额外的编码方式-(10%)) 鼓励使用额外的编码方式对索引进行压缩 (例如, gamma-encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验报告\n",
    "\n",
    "## 代码实现\n",
    "注：部分代码块的输出中有些数字，这些数字是输出的运行时间。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "1.**完善了IdMap类**，实现了str和int的转换，实现了当要检索的str不存在时，添加该内容。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "2.**实现了BSBIIndex的解析部分parse_block()并对其进行了测试。**遍历文件夹下的所有文件，建立包含所有文件的[term, doc]的列表td_pairs。测试了文件同名但处于不同块中时的情况，此时的文件编号应该是不同的。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "3.**实现了对.index文件的写操作。**\n",
    "\n",
    "（1）创建InvertedIndex的子类InvertedIndexWriter，使用这个子类的append()成员方法，实现对.index文件的添加操作，该方法主要讲term的倒排索引列表编码并写入.index文件中。\n",
    "\n",
    "（2）在BSBIIndex类中添加invert_write()成员方法，该方法遍历td_pairs中的所有内容，为所有term建立倒排索引列表，最后使用index.append()将结果写入.index文件中。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "4.**实现了各个块的合并操作。**\n",
    "\n",
    "（1）完善了InvertedIndexIterator类。\n",
    "\n",
    "在\\_\\_enter\\_\\_方法下，创建了对terms（倒排索引表中的词项）的迭代器，并让index_file的文件指针指向文件头。在\\_\\_next\\_\\_方法下，先获得下一个term，再利用metadata，从index_file中读出相应的倒排索引列表的编码。\n",
    "\n",
    "（2）利用heapq实现了BSBIIndex中的merge()方法。\n",
    "\n",
    "该方法利用最小堆，先将所有迭代器的第一个\\[ (term, postings_dict), i \\] （i是迭代器的下标）压入堆中，从而完成对最小堆的初始化。之后当堆不为空时，一直循环读堆顶的数据并压入相应的迭代器的下一对数据，当读到的堆顶的term与前一个不同时，将前面的postings_list做合并，并写入merge_index中，然后开始对下一个term做操作。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "5.**实现了文档的索引、查询操作。**\n",
    "\n",
    "（1）完善了InvertedIndexMapper类。\n",
    "\n",
    "使用term的metadata，将文件指针指到相应的位置，并读取对应的长度，将编码解码后返回。\n",
    "\n",
    "（2）实现了两个列表的交集操作。\n",
    "\n",
    "（3）实现了BSBIIndex的retrieve()成员函数。\n",
    "\n",
    "打开一个InvertedIndexMapper类，对query的每一个单词，利用InvertedIndexMapper类获得相应的postings_list，对所有倒排索引列表取交集，并返回文档的名称列表。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "6.**实现了可变长编码**（为了解决0的编码解码问题，本程序将所有docID都+1再进行解码，最后解码的结果也都-1）\n",
    "\n",
    "（1）编码\n",
    "\n",
    "使用位运算，对原来的docID二进制，每7位为1组进行编码。将postings_list中所有docID编码后拼接而成的内容转化为字节流后返回。\n",
    "\n",
    "（2）解码\n",
    "\n",
    "每8位进行解析，若首位为0则当前docID结束，进行下一个docID的计算。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "7.**实现了Gamma encoding**（为了解决0和1的编码解码问题，本程序将所有docID都+2再进行解码，最后解码的结果也都-2）\n",
    "\n",
    "（1）编码\n",
    "\n",
    "按docID一个个编码，然后将编码的结果拼接后变成字节流返回。\n",
    "对单个docID，获得该docID二进制的长度length，除去首位，将剩余的二进制左移length位，并将低(length-1)位置为1。\n",
    "\n",
    "注：gamma encoding原本应该是高位是一元编码，低位是原来的二进制除去首位，但为了后面解码方便，本程序将一元编码放在了低位。\n",
    "\n",
    "（2）解码\n",
    "\n",
    "先将字节流转换为整数，然后对整数进行解码。\n",
    "\n",
    "gamma encoding解码的关键在于找到第一位0。为加快解码速度，本程序每8位查找整数中的0，若低8位的值为255，说明这8位都是1，继续找下一个8位，若不为255，说明有0出现，这时再按2位2位找，最后在2位中找到为0的那一位。找到第一个0后，读取整数中相应的位数，加上首位的1，即得到一个docID。接下来就循环上述操作，直到整数为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add additional imports here\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验使用的文本数据是stanford.edu域下的网页内容，可从http://web.stanford.edu/class/cs276/pa/pa1-data.zip 下载。以下代码将大约170MB的文本数据下载到当前目录下，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/pa1-data.zip'\n",
    "data_dir = 'pa1-data'\n",
    "urllib.request.urlretrieve(data_url, data_dir+'.zip')\n",
    "zip_ref = zipfile.ZipFile(data_dir+'.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后构建的索引会被存储到`output_dir`，`tmp`会存储测试数据（toy-data）所生成的一些临时文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('toy_output_dir')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据目录下有10个子目录（命名0-9）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('pa1-data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个子目录下的文件都包含一个独立网页的内容。可以认为在同一子目录下没有同名文件，即每个文件的绝对路径不会相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir('pa1-data/0'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的网页内容已经经过处理，仅包含由空格分隔开的单词，不再需要进行额外的标准化工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pa1-data/0/3dradiology.stanford.edu_', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业目录下有一个小的数据集文件夹`toy-data`。在使用完整网页数据集之前，我们会用它来测试我们的代码是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dir = 'toy-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引构建与检索 (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业的第一部分是使用**blocked sort-based indexing (BSBI)** 算法来构建倒排索引并实现布尔检索。关于BSBI算法可以参考老师课件或者斯坦福教材[Section 4.2](http://nlp.stanford.edu/IR-book/pdf/04const.pdf)。以下摘自教材内容\n",
    "\n",
    "> To construct an index, we first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. \n",
    "\n",
    "对于无法在内存一次性处理的较大数据集，将会使用到二级存储（如：磁盘）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdMap\n",
    "\n",
    "再次引用教材 Section 4.2:\n",
    "\n",
    "> To make index construction more efficient, we represent terms as termIDs (instead of strings), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection. Similarly, we also represent documents as docIDs (instead of strings).\n",
    "\n",
    "我们首先定义一个辅助类`IdMap`，用于将字符串和数字ID进行相互映射，以满足我们在term和termID、doc和docID间转换的需求。\n",
    "\n",
    "实现以下代码中的`_get_str` 和 `_get_id`函数，IdMap类的唯一接口是`__getitem__`，它是一个特殊函数，重写了下标运算`[]`,根据下标运算键的类型得到正确的映射值（如果不存在需要添加）。（特殊函数可参考[官方文档](https://docs.python.org/3.7/reference/datamodel.html#special-method-names)）\n",
    "<br>\n",
    "<br>\n",
    "我们会用到字典来将字符串转换为数字，用列表来将数字转换为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if s not in self.str_to_id.keys():  # 没有在字典中，就分别在字典和list中添加\n",
    "            self.str_to_id[s] = len(self.id_to_str)\n",
    "            self.id_to_str.append(s)\n",
    "        return self.str_to_id[s]\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保代码能通过以下简单测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdMap = IdMap()\n",
    "assert testIdMap['a'] == 0, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['bcd'] == 1, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['a'] == 0, \"Unable to retrieve the id of an existing string\"\n",
    "assert testIdMap[1] == 'bcd', \"Unable to retrive the string corresponding to a\\\n",
    "                                given id\"\n",
    "try:\n",
    "    testIdMap[2]\n",
    "except IndexError as e:\n",
    "    assert True, \"Doesn't throw an IndexError for out of range numeric ids\"\n",
    "assert len(testIdMap) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后会需要你自己来写测试样例来确保你的程序正常运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将倒排列表编码成字节数组\n",
    "\n",
    "为了高效地从磁盘读写倒排列表（文档ID），我们将其存储为字节数组的形式。代码提供了`UncompressedPostings`类来用静态函数实现对倒排列表的编码和解码。在之后的任务中你需要使用该接口实现索引压缩版本（可变长编码）。\n",
    "\n",
    "参考:\n",
    "1. https://docs.python.org/3/library/array.html\n",
    "2. https://pymotw.com/3/array/#module-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncompressedPostings:\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes postings_list into a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            List of docIDs (postings)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bytes\n",
    "            bytearray representing integers in the postings_list\n",
    "        \"\"\"\n",
    "        # 转化为字节流，更方便得进行读写操作\n",
    "        return array.array('L', postings_list).tobytes()\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes postings_list from a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            bytearray representing encoded postings list as output by encode \n",
    "            function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded list of docIDs from encoded_postings_list\n",
    "        \"\"\"\n",
    "        \n",
    "        decoded_postings_list = array.array('L')\n",
    "        decoded_postings_list.frombytes(encoded_postings_list)\n",
    "        return decoded_postings_list.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码查看其工作方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      "12\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = UncompressedPostings.encode([1,2,3])\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(UncompressedPostings.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 磁盘上的倒排索引\n",
    "\n",
    "> With main memory insufficient, we need to use an external sorting algorithm, that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks. \n",
    "\n",
    "在这一部分我们提供了一个基类`InvertedIndex`，之后会在此基础上构建它的子类`InvertedIndexWriter`, `InvertedIndexIterator` 和 `InvertedIndexMapper`。在Python中我们常用`cPickle`进行序列化，但是它并不支持部分读和部分写，无法满足BSBI算法的需要，所以我们需要定义自己的存储方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"A class that implements efficient reads and writes of an inverted index \n",
    "    to disk\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    metadata_file: term，在索引文件中的起始位置，在这个list中包含的文件的个数，在这个list中编码的长度\n",
    "    postings_dict: Dictionary mapping: termID->(start_position_in_index_file, \n",
    "                                                number_of_postings_in_list,\n",
    "                                               length_in_bytes_of_postings_list)\n",
    "        This is a dictionary that maps from termIDs to a 3-tuple of metadata \n",
    "        that is helpful in reading and writing the postings in the index file \n",
    "        to/from disk. This mapping is supposed to be kept in memory. \n",
    "        start_position_in_index_file is the position (in bytes) of the postings \n",
    "        list in the index file\n",
    "        number_of_postings_in_list is the number of postings (docIDs) in the \n",
    "        postings list\n",
    "        length_in_bytes_of_postings_list is the length of the byte \n",
    "        encoding of the postings list\n",
    "    \n",
    "    index_file: item按顺序（文件在postings_list中的顺序）排列\n",
    "    terms: List[int]\n",
    "        A list of termIDs to remember the order in which terms and their \n",
    "        postings lists were added to index. \n",
    "        \n",
    "        After Python 3.7 we technically no longer need it because a Python dict \n",
    "        is an OrderedDict, but since it is a relatively new feature, we still\n",
    "        maintain backward compatibility with a list to keep track of order of \n",
    "        insertion. \n",
    "    \"\"\"\n",
    "    def __init__(self, index_name, postings_encoding = None, directory=''):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_name (str): Name used to store files related to the index\n",
    "        postings_encoding: A class implementing static methods for encoding and \n",
    "            decoding lists of integers. Default is None, which gets replaced\n",
    "            with UncompressedPostings\n",
    "        directory (str): Directory where the index files will be stored\n",
    "        \"\"\"\n",
    "\n",
    "        self.index_file_path = os.path.join(directory, index_name+'.index')\n",
    "        self.metadata_file_path = os.path.join(directory, index_name+'.dict')\n",
    "\n",
    "        if postings_encoding is None:\n",
    "            self.postings_encoding = UncompressedPostings\n",
    "        else:\n",
    "            self.postings_encoding = postings_encoding\n",
    "        self.directory = directory\n",
    "\n",
    "        self.postings_dict = {}  # 倒排索引 三元组\n",
    "        self.terms = []         #Need to keep track of the order in which the \n",
    "                                #terms were inserted. Would be unnecessary \n",
    "                                #from Python 3.7 onwards\n",
    "                                # 包含的terms\n",
    "    \n",
    "    \"\"\"enter()和exit()的作用是可以把类当成文件进行IO操作\"\"\"\n",
    "    # 使用with语句时会进行enter()\n",
    "    def __enter__(self):\n",
    "        \"\"\"Opens the index_file and loads metadata upon entering the context\"\"\"\n",
    "        # Open the index file\n",
    "        self.index_file = open(self.index_file_path, 'rb+')\n",
    "\n",
    "        # Load the postings dict and terms from the metadata file\n",
    "        with open(self.metadata_file_path, 'rb') as f:\n",
    "            self.postings_dict, self.terms = pkl.load(f)\n",
    "            self.term_iter = self.terms.__iter__()                       \n",
    "\n",
    "        return self\n",
    "    \n",
    "    # 退出with时会进行exit()\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Closes the index_file and saves metadata upon exiting the context\"\"\"\n",
    "        # Close the index file\n",
    "        self.index_file.close()\n",
    "        \n",
    "        # Write the postings dict and terms to the metadata file\n",
    "        with open(self.metadata_file_path, 'wb') as f:\n",
    "            pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是在与磁盘上的文件进行交互，所以我们提供了`__enter__`和`__exit__`函数，它使得我们能够像使用python中文件IO一样使用`with`语句。（参考[上下文管理器官方文档](https://docs.python.org/3/library/contextlib.html)）\n",
    "\n",
    "以下是使用 `InvertedIndexWriter` 上下文管理器的实例:\n",
    "\n",
    "```python\n",
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    # Some code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引\n",
    "\n",
    "> BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. \n",
    "\n",
    "你需要将每一个子目录当做一个块（block），并且在构建索引的过程中每次只能加载一个块（模拟内存不足）。注意到我们是将操作系统意义上的块进行了抽象。你可以认为每个块足够小，能被装载进内存。\n",
    "\n",
    "在这一部分，我们将阶段性地构造类`BSBIIndex`。函数`index`给出了BSBI算法的框架，而你的工作则是在接下来的部分中实现函数`parse_block`, `invert_write` 和 `merge`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not make any changes here, they will be overwritten while grading\n",
    "class BSBIIndex:\n",
    "    \"\"\" \n",
    "    Attributes\n",
    "    ----------\n",
    "    term_id_map(IdMap): For mapping terms to termIDs\n",
    "    doc_id_map(IdMap): For mapping relative paths of documents (eg \n",
    "        0/3dradiology.stanford.edu_) to docIDs\n",
    "    data_dir(str): Path to data\n",
    "    output_dir(str): Path to output index files\n",
    "    index_name(str): Name assigned to index\n",
    "    postings_encoding: Encoding used for storing the postings.\n",
    "        The default (None) implies UncompressedPostings\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, output_dir, index_name = \"BSBI\", \n",
    "                 postings_encoding = None):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.index_name = index_name\n",
    "        self.postings_encoding = postings_encoding  # 编码类型\n",
    "\n",
    "        # Stores names of intermediate indices\n",
    "        self.intermediate_indices = []\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Dumps doc_id_map and term_id_map into output directory\"\"\"\n",
    "        # 保存doc_id_map和term_id_map到相应的输出目录下\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'wb') as f:\n",
    "            pkl.dump(self.term_id_map, f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'wb') as f:\n",
    "            pkl.dump(self.doc_id_map, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Loads doc_id_map and term_id_map from output directory\"\"\"\n",
    "        # 从相应的输出目录下读取doc_id_map和term_id_map\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'rb') as f:\n",
    "            self.term_id_map = pkl.load(f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'rb') as f:\n",
    "            self.doc_id_map = pkl.load(f)\n",
    "            \n",
    "    def index(self):\n",
    "        \"\"\"Base indexing code\n",
    "        \n",
    "        This function loops through the data directories, \n",
    "        calls parse_block to parse the documents  使用parse_block解析文档\n",
    "        calls invert_write, which inverts each block and writes to a new index  使用invert_write构建倒排索引表\n",
    "        then saves the id maps and calls merge on the intermediate indices\n",
    "        \"\"\"\n",
    "        time_start=time.time()\n",
    "        i = 0\n",
    "        # 遍历所有数据文件\n",
    "        for block_dir_relative in sorted(next(os.walk(self.data_dir))[1]):  # next() 返回迭代器的下一个项目\n",
    "            # block_dir_relative: 块的相对路径\n",
    "            start = time.time()\n",
    "            td_pairs = self.parse_block(block_dir_relative)  # 对文档进行解析，将文档转换为id对   获得term和doc的映射\n",
    "            index_id = 'index_'+block_dir_relative\n",
    "            self.intermediate_indices.append(index_id)  # 就是list的append\n",
    "            with InvertedIndexWriter(index_id, directory=self.output_dir, \n",
    "                                     postings_encoding=\n",
    "                                     self.postings_encoding) as index:  # index是一个InvertedIndexWriter类的实例\n",
    "                self.invert_write(td_pairs, index)  # 使用td_pairs写一个倒排索引表,将表写入index中\n",
    "                td_pairs = None\n",
    "            end = time.time()\n",
    "            print(\"第\"+str(i)+\"块时间为: \", end-start)  # 输出当前块的时间\n",
    "            i += 1\n",
    "        self.save()  # 保留两个映射\n",
    "        time_end=time.time()\n",
    "        print('索引构建时间：', time_end-time_start)  # 输出所有快构建的总时间\n",
    "        time_start=time.time()\n",
    "        with InvertedIndexWriter(self.index_name, directory=self.output_dir, \n",
    "                                 postings_encoding=\n",
    "                                 self.postings_encoding) as merged_index:  # merged_index是一个InvertedIndexWriter类的实例\n",
    "            # 使用contextlib可以同时打开多个文件\n",
    "            with contextlib.ExitStack() as stack:\n",
    "                # indices是InvertedIndexIterator类实例的列表\n",
    "                indices = [stack.enter_context(\n",
    "                    InvertedIndexIterator(index_id, \n",
    "                                          directory=self.output_dir, \n",
    "                                          postings_encoding=\n",
    "                                          self.postings_encoding)) \n",
    "                 for index_id in self.intermediate_indices]\n",
    "                self.merge(indices, merged_index)  # 很多个小的索引文件合并成大的索引文件\n",
    "        time_end=time.time()\n",
    "        print('索引合并时间：', time_end-time_start)  # 输出合并的时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析\n",
    "\n",
    "> The function `parse_block`  parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full. We choose the block size to fit comfortably into memory to permit a fast in-memory sort. \n",
    "\n",
    "你需要将每一个子目录当做一个块，`parse_block`接收子目录路径作为参数。同一子目录下所有文件名都是不同的。\n",
    "\n",
    "_注意 - 我们使用 `BSBIIndex` 继承 `BSBIIndex`, 这只是对一个已存在类添加新内容的简单方法。在这里只是用来切分类的定义（jupyter notebook内教学使用，无特殊含义）。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):  \n",
    "    # 文档解析\n",
    "    def parse_block(self, block_dir_relative):\n",
    "        \"\"\"Parses a tokenized text file into termID-docID pairs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        block_dir_relative : str\n",
    "            Relative Path to the directory that contains the files for the block\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[Int, Int]]\n",
    "            Returns all the td_pairs extracted from the block\n",
    "        \n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        These persist across calls to parse_block\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        td_pairs = []  # 初始化结果list\n",
    "        son_data_dir = os.path.join(self.data_dir, block_dir_relative)  # 子目录路径\n",
    "        # 获得子目录中所有文件的list\n",
    "        for root, dirs, files in os.walk(son_data_dir):\n",
    "            # 遍历所有文件\n",
    "            for text_file in sorted(files):\n",
    "                text_file_dir = os.path.join(son_data_dir, text_file)\n",
    "                with open(text_file_dir, 'r') as f:\n",
    "                    text = f.read().split()\n",
    "                # 构建td列表\n",
    "                for word in set(text):\n",
    "                    td_pairs.append((self.term_id_map[word], self.doc_id_map[os.path.join(block_dir_relative, text_file)]))\n",
    "        return td_pairs\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察函数在测试数据上是否正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm fine , thank you\n",
      "\n",
      "hi hi\n",
      "how are you ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('toy-data/0/fine.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "with open('toy-data/0/hello.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (4, 0),\n",
       " (5, 1),\n",
       " (2, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_dir = 'toy-data/'\n",
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "BSBI_instance.parse_block('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一些测试样例来确保`parse_block`方法正常运行（如：相同单词出现时是相同ID）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (5, 3),\n",
       " (2, 3),\n",
       " (6, 3),\n",
       " (7, 3),\n",
       " (8, 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Begin your code\n",
    "BSBI_instance.parse_block('2')  # 测试文件同名但目录不同的情况\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 倒排表\n",
    "\n",
    "> The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk.\n",
    "\n",
    "在这一部分我们添加函数`invert_write`来实现由termID-docID对构建倒排表。 \n",
    "\n",
    "但是，我们首先需要实现类`InvertedIndexWriter`。和列表类似，该类提供了append函数，但是倒排表不会存储在内存中而是直接写入到磁盘里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexWriter(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.index_file = open(self.index_file_path, 'wb+')              \n",
    "        return self\n",
    "\n",
    "    def append(self, term, postings_list):\n",
    "        \"\"\"Appends the term and postings_list to end of the index file.\n",
    "        \n",
    "        This function does three things, \n",
    "        1. Encodes the postings_list using self.postings_encoding\n",
    "        2. Stores metadata in the form of self.terms and self.postings_dict\n",
    "           Note that self.postings_dict maps termID to a 3 tuple of \n",
    "           (start_position_in_index_file, \n",
    "           number_of_postings_in_list, \n",
    "           length_in_bytes_of_postings_list)\n",
    "        3. Appends the bytestream to the index file on disk\n",
    "\n",
    "        Hint: You might find it helpful to read the Python I/O docs\n",
    "        (https://docs.python.org/3/tutorial/inputoutput.html) for\n",
    "        information about appending to the end of a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term:\n",
    "            term or termID is the unique identifier for the term\n",
    "        postings_list: List[Int]\n",
    "            List of docIDs where the term appears\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        # 需要结合with使用\n",
    "        postings_list = sorted(postings_list)  # postings_list理应是有序的，但为了安全起见，还是再排一遍序\n",
    "        after_encoding = self.postings_encoding.encode(postings_list)  # 获得编码后的bytes\n",
    "        start_position_in_index_file = self.index_file.tell()  # 当前文档的长度\n",
    "        number_of_postings_in_list = len(postings_list)  # 列表的长度\n",
    "        length_in_bytes_of_postings_list = len(after_encoding)  # bytes的长度\n",
    "        self.terms.append(term)  # 添加这个term\n",
    "        # 添加这个term的metadata\n",
    "        self.postings_dict[term] = (start_position_in_index_file, number_of_postings_in_list, length_in_bytes_of_postings_list)\n",
    "        self.index_file.write(after_encoding)  # 将编码后的内容写入.index文件\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管还没有实现读取索引的类，我们还是可以用以下测试代码检测我们的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "    index.index_file.seek(0)\n",
    "    # print(index.terms)\n",
    "    # print(index.postings_dict)\n",
    "    assert index.terms == [1,2], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict == {1: (0, 3, len(UncompressedPostings.encode([2,3,4]))), \n",
    "                                   2: (len(UncompressedPostings.encode([2,3,4])), 3, \n",
    "                                       len(UncompressedPostings.encode([3,4,5])))}, \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [2, 3, 4, 3, 4, 5], \"postings on disk incorrect\"\n",
    "\n",
    "# 确定terms和postings_dict是否保存\n",
    "with InvertedIndex('test', directory='tmp/') as index:\n",
    "    index.index_file.seek(0)\n",
    "    assert index.terms == [1,2], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict == {1: (0, 3, len(UncompressedPostings.encode([2,3,4]))), \n",
    "                                   2: (len(UncompressedPostings.encode([2,3,4])), 3, \n",
    "                                       len(UncompressedPostings.encode([3,4,5])))}, \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [2, 3, 4, 3, 4, 5], \"postings on disk incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们实现 `invert_write`，它将解析得到的td_pairs转换成倒排表，并使用`InvertedIndexWriter` 类将其写入磁盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):\n",
    "    def invert_write(self, td_pairs, index):\n",
    "        \"\"\"Inverts td_pairs into postings_lists and writes them to the given index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        td_pairs: List[Tuple[Int, Int]]\n",
    "            List of termID-docID pairs\n",
    "        index: InvertedIndexWriter\n",
    "            Inverted index on disk corresponding to the block       \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        postings_list = {}\n",
    "        # 为所有term在字典中创建空列表\n",
    "        for term in [x[0] for x in td_pairs]:\n",
    "            postings_list[term] = []\n",
    "        # 获得所有term的倒排索引表\n",
    "        for term, doc in td_pairs:\n",
    "            postings_list[term].append(doc)\n",
    "        # 将倒排表写入.index文件中\n",
    "        for term in sorted(postings_list.keys()):\n",
    "            index.append(term, postings_list[term])\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在测试数据上读取一个块并观察倒排索引中包含的内容。\n",
    "仿照`InvertedIndexWriter`部分写一些测试样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 1), (2, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "{0: (0, 1, 4), 1: (4, 1, 4), 2: (8, 2, 8), 3: (16, 1, 4), 4: (20, 1, 4), 5: (24, 1, 4), 6: (28, 1, 4), 7: (32, 1, 4), 8: (36, 1, 4)}\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "block_dir_relative = '0'\n",
    "td_pairs = BSBI_instance.parse_block(block_dir_relative)\n",
    "print(td_pairs)\n",
    "index_id = block_dir_relative\n",
    "with InvertedIndexWriter(index_id, directory = BSBI_instance.output_dir, \n",
    "                         postings_encoding = BSBI_instance.postings_encoding) as index:\n",
    "    BSBI_instance.invert_write(td_pairs, index)\n",
    "    index.index_file.seek(0)\n",
    "    print(index.postings_encoding.decode(index.index_file.read()))\n",
    "with InvertedIndex(index_id, directory = BSBI_instance.output_dir, \n",
    "                   postings_encoding = BSBI_instance.postings_encoding) as index:\n",
    "    print(index.terms)\n",
    "    print(index.postings_dict)\n",
    "    print(index.postings_encoding.decode(index.index_file.read()))\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并\n",
    "> The algorithm simultaneously merges the ten blocks into one large merged index. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. \n",
    "\n",
    "Python中的迭代模型非常自然地符合我们维护一个小的读缓存的要求。我们可以迭代地从磁盘上每次读取文件的一个倒排列表。我们通过构建`InvertedIndex`的子类`InvertedIndexIterator`来完成这个迭代任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexIterator(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        \"\"\"Adds an initialization_hook to the __enter__ function of super class\n",
    "        \"\"\"\n",
    "        super().__enter__()\n",
    "        self._initialization_hook()\n",
    "        return self\n",
    "\n",
    "    def _initialization_hook(self):\n",
    "        \"\"\"Use this function to initialize the iterator\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        self.term_iterator = iter(self.terms)  # 创建对倒排索引表中词项的迭代器\n",
    "        self.index_file.seek(0)  # 令文件指针指向文件头\n",
    "        ### End your code\n",
    "\n",
    "    def __iter__(self): \n",
    "        return self\n",
    "    \n",
    "    def __next__(self):  # 迭代操作\n",
    "        \"\"\"Returns the next (term, postings_list) pair in the index.\n",
    "        \n",
    "        Note: This function should only read a small amount of data from the \n",
    "        index file. In particular, you should not try to maintain the full \n",
    "        index file in memory.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        (start_position_in_index_file, \n",
    "           number_of_postings_in_list, \n",
    "           length_in_bytes_of_postings_list)\"\"\"\n",
    "        ### Begin your code\n",
    "        this_term = next(self.term_iterator)  # 获得下一个词项\n",
    "        # 获得该词项的倒排索引列表的编码长度\n",
    "        length_in_bytes_of_postings_list = self.postings_dict[this_term][2]\n",
    "        # 从文件中读取相应长度\n",
    "        postings_list = self.index_file.read(length_in_bytes_of_postings_list)\n",
    "        return (this_term, self.postings_encoding.decode(postings_list))\n",
    "        ### End your code\n",
    "\n",
    "    def delete_from_disk(self):  # 从磁盘上删除小的文件\n",
    "        \"\"\"Marks the index for deletion upon exit. Useful for temporary indices\n",
    "        \"\"\"\n",
    "        self.delete_upon_exit = True\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Delete the index file upon exiting the context along with the\n",
    "        functions of the super class __exit__ function\"\"\"\n",
    "        self.index_file.close()\n",
    "        if hasattr(self, 'delete_upon_exit') and self.delete_upon_exit:\n",
    "            os.remove(self.index_file_path)\n",
    "            os.remove(self.metadata_file_path)\n",
    "        else:\n",
    "            with open(self.metadata_file_path, 'wb') as f:\n",
    "                pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了测试以上代码，我们先用`InvertedIndexWriter` 创建索引，然后再迭代遍历。写一些小的测试样例观察它是否正常运行。至少你得写一个测试，手工构建一个小的索引，用`InvertedIndexWriter`将他们写入磁盘，然后用`InvertedIndexIterator`迭代遍历倒排索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 1), (2, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n",
      "(0, [0])\n",
      "(1, [0])\n",
      "(2, [0, 1])\n",
      "(3, [0])\n",
      "(4, [0])\n",
      "(5, [1])\n",
      "(6, [1])\n",
      "(7, [1])\n",
      "(8, [1])\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "block_dir_relative = '0'\n",
    "td_pairs = BSBI_instance.parse_block(block_dir_relative)\n",
    "print(td_pairs)\n",
    "index_id = block_dir_relative\n",
    "with InvertedIndexWriter(index_id, directory = BSBI_instance.output_dir, \n",
    "                         postings_encoding = BSBI_instance.postings_encoding) as index:\n",
    "    BSBI_instance.invert_write(td_pairs, index)\n",
    "    index.index_file.seek(0)\n",
    "    print(UncompressedPostings.decode(index.index_file.read()))\n",
    "    with InvertedIndexIterator(index_id, directory = BSBI_instance.output_dir, \n",
    "                         postings_encoding = BSBI_instance.postings_encoding) as Iterator:\n",
    "        while(1):\n",
    "            try:\n",
    "                print(next(Iterator))\n",
    "            except:\n",
    "                break\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> During merging, in each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary.\n",
    "\n",
    "我们将使用`InvertedIndexIterator`来完成读取的部分，用`InvertedIndexWriter`来将合并后的索引写入磁盘。\n",
    "\n",
    "注意到我们之前一直使用`with` 语句来打开倒排索引文件，但是现在需要程序化地完成这项工作。可以看到我们在基类`BSBIIndex`的`index`函数中使用了`contextlib`([参考文档](https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack))\n",
    "你的任务是合并*打开的*`InvertedIndexIterator`对象（倒排列表），并且通过一个`InvertedIndexWriter`对象每次写入一个文档列表。\n",
    "\n",
    "既然我们知道文档列表已经排过序了，那么我们可以在线性时间内对它们进行合并排序。事实上`heapq`([参考文档](https://docs.python.org/3.0/library/heapq.html)) 是Python中一个实现了堆排序算法的标准模板。另外它还包含一个实用的函数`heapq.merge`，可以将多个已排好序的输入（可迭代）合并成一个有序的输出并返回它的迭代器。它不仅可以用来合并倒排列表，也可以合并倒排索引词典。\n",
    "\n",
    "为了让你快速上手`heapq.merge`函数，我们提供了一个简单的使用样例。给出两个以动物和树平均寿命排序的列表，我们希望合并它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Giraffe', 28)\n",
      "('Rhinoceros', 40)\n",
      "('Gray Birch', 50)\n",
      "('Gray Birch', 50)\n",
      "('Indian Elephant', 70)\n",
      "('Black Willow', 70)\n",
      "('Golden Eagle', 80)\n",
      "('Basswood', 100)\n",
      "('Box turtle', 123)\n",
      "('Bald Cypress', 600)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import heapq  # 实现堆排序的数据结构\n",
    "animal_lifespans = [('Giraffe', 28), \n",
    "                   ('Rhinoceros', 40), \n",
    "                   ('Gray Birch', 50),\n",
    "                   ('Indian Elephant', 70), \n",
    "                   ('Golden Eagle', 80), \n",
    "                   ('Box turtle', 123)]\n",
    "\n",
    "tree_lifespans = [('Gray Birch', 50), \n",
    "                  ('Black Willow', 70), \n",
    "                  ('Basswood', 100),\n",
    "                  ('Bald Cypress', 600)]\n",
    "\n",
    "lifespan_lists = [animal_lifespans, tree_lifespans]\n",
    "\n",
    "for merged_item in heapq.merge(*lifespan_lists, key=lambda x: x[1]):\n",
    "    print(merged_item)\n",
    "\n",
    "test_list = [[1,6,7], [2,3,4]]\n",
    "for item in heapq.merge(*test_list):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意使用`*`将`lifespan_lists`解包作为参数，并且使用`lambda`函数来给出进行排序的key。如果你对它们不熟悉可以参考文档([unpacking lists](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists), [lambda expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def merge(self, indices, merged_index):\n",
    "        \"\"\"Merges multiple inverted indices into a single index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: List[InvertedIndexIterator]\n",
    "            A list of InvertedIndexIterator objects, each representing an\n",
    "            iterable inverted index for a block\n",
    "        merged_index: InvertedIndexWriter\n",
    "            An instance of InvertedIndexWriter object into which each merged \n",
    "            postings list is written out one at a time\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        block_number = len(indices)  # 块的个数\n",
    "        heap = []  # 要维护的最小堆\n",
    "        # 初始化最小堆\n",
    "        for i in range(block_number):\n",
    "            heappush(heap, [next(indices[i]), i])\n",
    "        # 当堆不为空时\n",
    "        while heap:\n",
    "            cur_all_postings = []  # 文档列表的list\n",
    "            cur_postings = heappop(heap)  # 获得堆顶\n",
    "            # 若对应的迭代器不为空，则压入堆中，否则将对应文件标志为删除\n",
    "            try:\n",
    "                heappush(heap, [next(indices[cur_postings[1]]), cur_postings[1]])\n",
    "            except:\n",
    "                indices[cur_postings[1]].delete_from_disk()\n",
    "            # 获得当前词项的编号\n",
    "            cur_item = cur_postings[0][0]\n",
    "            cur_all_postings.append(cur_postings[0][1])\n",
    "            # 使用最小堆，获得所有当前词项的文档列表\n",
    "            while heap :\n",
    "                # 堆顶的词项不是当前词项，则退出\n",
    "                if cur_item != heap[0][0][0]:\n",
    "                    break\n",
    "                postings = heappop(heap)\n",
    "                cur_all_postings.append(postings[0][1])\n",
    "                try:\n",
    "                    heappush(heap, [next(indices[postings[1]]), postings[1]])\n",
    "                except:\n",
    "                    indices[postings[1]].delete_from_disk()\n",
    "            # 合并\n",
    "            merged_index.append(cur_item, [x for x in heapq.merge(*cur_all_postings)])\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先确保它在测试数据上正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0块时间为:  0.0039310455322265625\n",
      "第1块时间为:  0.0225677490234375\n",
      "第2块时间为:  0.004025697708129883\n",
      "索引构建时间： 0.033577919006347656\n",
      "索引合并时间： 0.03691291809082031\n"
     ]
    }
   ],
   "source": [
    "toy_BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'toy_output_dir', postings_encoding = UncompressedPostings)\n",
    "toy_BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对整个数据集构建索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0块时间为:  22.949820280075073\n",
      "第1块时间为:  20.4154269695282\n",
      "第2块时间为:  24.456767320632935\n",
      "第3块时间为:  21.595614910125732\n",
      "第4块时间为:  23.422343730926514\n",
      "第5块时间为:  20.017051458358765\n",
      "第6块时间为:  18.818127632141113\n",
      "第7块时间为:  16.056620597839355\n",
      "第8块时间为:  16.177923440933228\n",
      "第9块时间为:  15.2453293800354\n",
      "索引构建时间： 199.5573170185089\n",
      "索引合并时间： 22.398460865020752\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', postings_encoding = UncompressedPostings)\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你在合并阶段出现了错误，可以利用以下代码来debug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', postings_encoding = UncompressedPostings)\n",
    "BSBI_instance.intermediate_indices = ['index_'+str(i) for i in range(10)]\n",
    "with InvertedIndexWriter(BSBI_instance.index_name, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding) as merged_index:\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        indices = [stack.enter_context(InvertedIndexIterator(index_id, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding)) for index_id in BSBI_instance.intermediate_indices]\n",
    "        BSBI_instance.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 布尔联合检索 (10%)\n",
    "\n",
    "我们将实现BSBIIndex中的`retrieve`函数，对于给定的包含由空格分隔tokens的字符串查询，返回包含查询中所有tokens的文档列表。但是我们并不能迭代遍历整个索引或者将整个索引加载到内存来寻找相应的terms（索引太大）。\n",
    "\n",
    "首先我们要实现`InvertedIndex`的子类`InvertedIndexMapper`，它能够找到对应terms在索引文件中位置并取出它的倒排记录表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexMapper(InvertedIndex):\n",
    "    def __getitem__(self, key):\n",
    "        return self._get_postings_list(key)\n",
    "    \n",
    "    def _get_postings_list(self, term):\n",
    "        \"\"\"Gets a postings list (of docIds) for `term`.\n",
    "        \n",
    "        This function should not iterate through the index file.\n",
    "        I.e., it should only have to read the bytes from the index file\n",
    "        corresponding to the postings list for the requested term.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        # 获得当前term的metadata\n",
    "        metadata = self.postings_dict[term]\n",
    "        self.index_file.seek(metadata[0])  # 将文件指针定位到对应的位置\n",
    "        # 读取相应的长度，解码后返回\n",
    "        return self.postings_encoding.decode(self.index_file.read(metadata[2]))\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一些测试样例检测`_get_postings_list`的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 2\n",
      "[0, 1, 2, 4, 5]\n",
      "bye 9\n",
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "query = ['you', 'bye']\n",
    "with InvertedIndexMapper(index_name = 'BSBI', directory = 'toy_output_dir') as Mapper:\n",
    "    for word in query:\n",
    "        print(word, toy_BSBI_instance.term_id_map[word])\n",
    "        print(Mapper[toy_BSBI_instance.term_id_map[word]])\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们获得了查询中terms对应的倒排记录表，接着需要求他们的交集。这部分与我们之前作业的merge方法类似，请实现`sorted_intersect`函数，遍历两个有序列表并在线性时间内合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_intersect(list1, list2):\n",
    "    \"\"\"Intersects two (ascending) sorted lists and returns the sorted result\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: List[Comparable]\n",
    "    list2: List[Comparable]\n",
    "        Sorted lists to be intersected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Comparable]\n",
    "        Sorted intersection        \n",
    "    \"\"\"\n",
    "    ### Begin your code\n",
    "    i = 0\n",
    "    j = 0\n",
    "    result = []\n",
    "    while(i < len(list1) and j < len(list2)):\n",
    "        if list1[i] > list2[j]:\n",
    "            j += 1\n",
    "        elif list1[i] < list2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "    return result\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "print(sorted_intersect([0,1,7,9,10], [0,2,3,10]))\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以利用`sorted_intersect` 和 `InvertedIndexMapper`来实现`retrieve`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%tee submission/retrieve.py\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def retrieve(self, query):\n",
    "        \"\"\"Retrieves the documents corresponding to the conjunctive query\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "            Space separated list of query tokens\n",
    "            \n",
    "        Result\n",
    "        ------\n",
    "        List[str]\n",
    "            Sorted list of documents which contains each of the query tokens. \n",
    "            Should be empty if no documents are found.\n",
    "        \n",
    "        Should NOT throw errors for terms not in corpus\n",
    "        \"\"\"\n",
    "        if len(self.term_id_map) == 0 or len(self.doc_id_map) == 0:\n",
    "            self.load()\n",
    "\n",
    "        ### Begin your code\n",
    "        DocIds = []\n",
    "        result = []\n",
    "        with InvertedIndexMapper(index_name = self.index_name, directory = self.output_dir, \n",
    "                                 postings_encoding=self.postings_encoding) as Mapper:\n",
    "            # 遍历query中的所有单词\n",
    "            for word in set(query.split()):\n",
    "                if self.term_id_map[word] not in Mapper.terms:\n",
    "                    DocIds = []\n",
    "                    break\n",
    "                this_docId = Mapper[self.term_id_map[word]]\n",
    "                if DocIds != []:\n",
    "                    DocIds = sorted_intersect(DocIds, this_docId)\n",
    "                else:\n",
    "                    DocIds = this_docId\n",
    "                if DocIds == []:\n",
    "                    break\n",
    "        for doc_Id in DocIds:\n",
    "            result.append(self.doc_id_map[doc_Id])\n",
    "        return result\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过一个简单的查询来测试索引在真实数据集上是否正常工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\\\fine.txt', '2\\\\fine.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_BSBI_instance = BSBIIndex(data_dir='toy-data', output_dir = 'toy_output_dir', postings_encoding = UncompressedPostings)\n",
    "toy_BSBI_instance.retrieve('thank you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\\\cs276.stanford.edu_',\n",
       " '1\\\\cs276a.stanford.edu_',\n",
       " '3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " '6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " '8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', postings_encoding = UncompressedPostings)\n",
    "BSBI_instance.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以通过读取文件来测试其中的页面是否真的包含了查询的terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs276 information retrieval and web search cs 276 ling 286 information retrieval and web search spring 2011 pandu nayak and prabhakar raghavan lecture 3 units tu th 4 15 5 30 gates b03 tas sonali aggarwal ivan cui valentin spitkovsky and sandeep sripada staff e mail cs276 spr1011 staff lists stanford edu lectures are also available online and on television through scpd sitn course description basic and advanced techniques for text based information systems efficient text indexing boolean and vector space retrieval models evaluation and interface issues web search including crawling link based algorithms and web metadata text web clustering classification text mining syllabus additional information staff contact information piazzza we strongly recommend students to post questions to the course page on www piazzza com instead of sending emails this forum enables students to discuss the questions they encounter in lectures or assignments here is a quick introduction video email if you have a question not appropriate for the piazzza forum eg one that is only relevant to your situation or one that reveals part of your solution to a homework question please email the staff mailing list at cs276 spr1011 staff lists stanford edu professor pandu nayak office none on campus office hours by appointment email nayak cs stanford edu professor prabhakar raghavan office none on campus office hours by appointment email pragh cs stanford edu ta sonali aggarwal office location b26 a ph 650 723 6319 office hours wednesday 4 30pm 6 30pm email sonali9 stanford edu ta ivan cui office location b26 b ph 650 736 1817 office hours monday 2 15pm 4 15pm email ivancui stanford edu ta valentin spitkovsky office location gates 228 second floor office hours thursday 2 15pm 4 15pm email vals stanford edu ta sandeep sripada office location b26 a ph 650 723 6319 office hours tuesday 5 30pm 7 30pm remember to carry your stanford id to open the basement doors email sss cs stanford edu announcements if you are not registered in the course but wish to receive course announcements you may subscribe to the guest mailing list cs276 spr1011 guests lists stanford edu textbook introduction to information retrieval by c manning p raghavan and h schutze cambridge university press available from the stanford bookstore or other fine retailers you can also download and print chapters at the book website prerequisites cs 103b and cs 107 and any one of cs 121 cs 145 or cs 161 or equivalent background programming experience will be necessary for the two practical exercises related courses introduction to computational advertising http www stanford edu class msande239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"pa1-data/1/cs276.stanford.edu_\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试dev queries（提前构建好的查询与结果）是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match for query: we are\n",
      "Results match for query: stanford class\n",
      "Results match for query: stanford students\n",
      "Results match for query: very cool\n",
      "Results match for query: the\n",
      "Results match for query: a\n",
      "Results match for query: the the\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = sorted([os.path.normpath(path) for path in BSBI_instance.retrieve(query)])\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果出现了错误，可以通过以下代码比较输出与正确结果的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(my_results) - set(reference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(reference_results) - set(my_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保你构建自己的查询来测试所有可能的边界情况，例如数据集中没有出现的terms，或者拖慢合并的频繁出现的terms。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引压缩  (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分，你将使用可变长字节编码对索引进行压缩。（gap-encoding可选，对gap进行编码）\n",
    "\n",
    "下面几个Python运算符可能对你有帮助"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % 2 =  0\n",
      "10 % 3 =  1\n",
      "10 / 3 =  3.3333333333333335\n",
      "10 // 3 =  3\n"
     ]
    }
   ],
   "source": [
    "# Remainder (modulo) operator %\n",
    "\n",
    "print(\"10 % 2 = \", 10 % 2)\n",
    "print(\"10 % 3 = \", 10 % 3)\n",
    "\n",
    "# Integer division in Python 3 is done by using two slash signs\n",
    "\n",
    "print(\"10 / 3 = \", 10 / 3)\n",
    "print(\"10 // 3 = \", 10 // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成下面的`CompressedPostings`类，我们将用它来替换`UncompressedPostings`。关于如何使用gap-encoding和可变长字节编码，你可以参考老师课件或者[Chapter 5](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    @staticmethod\n",
    "    def do_number_encoding(num):\n",
    "        num += 1  # 为了编码0, 所有数字加1\n",
    "        length = 1\n",
    "        div = 2 ** 7\n",
    "        result = num % div\n",
    "        num //= div\n",
    "        # 数字不为0时，一直循环\n",
    "        while num > 0:\n",
    "            reminder = num % div  # 7位为1组\n",
    "            num //= div  # 整数左移7位\n",
    "            reminder = reminder | div  # 或上 1000 0000\n",
    "            result = result << 8\n",
    "            result = reminder | result\n",
    "            length += 1\n",
    "        return length, result\n",
    "    \n",
    "    @staticmethod\n",
    "    def do_number_decoding(num):\n",
    "        div = 2 ** 8\n",
    "        highest_pos = 2 ** 7\n",
    "        result = []\n",
    "        this_number = 0\n",
    "        while num > 0:\n",
    "            reminder = num % div\n",
    "            this_number = this_number << 7\n",
    "            this_number = this_number | (reminder & (highest_pos - 1))\n",
    "            # 当前posting的最后一位\n",
    "            if reminder < highest_pos:\n",
    "                result.append(this_number - 1)  # 插入当前数字，将原先增加的1减去\n",
    "                this_number = 0\n",
    "            num //= div\n",
    "        result.reverse()\n",
    "        return result\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` using gap encoding with variable byte \n",
    "        encoding for each gap\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "            (as produced by `array.tobytes` function)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        result = 0\n",
    "        all_len = 0  # 记录字节的长度\n",
    "        for docId in postings_list:\n",
    "            length, binary = CompressedPostings.do_number_encoding(docId)\n",
    "            result = (result << (length * 8) ) | binary  # 将之前的结果和现在的结果拼接\n",
    "            all_len += length\n",
    "        return result.to_bytes(all_len, byteorder='big')\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docIds)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        # 从字节流转换为整数\n",
    "        num = int.from_bytes(encoded_postings_list, byteorder='big')\n",
    "        decoded_postings_list = CompressedPostings.do_number_decoding(num)\n",
    "        return decoded_postings_list\n",
    "        ### End your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，如果你实现了额外的辅助函数，写一些测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x02\\x03\\x04'\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "encoding_test = CompressedPostings()\n",
    "encoding_result = encoding_test.encode([1, 2, 3])\n",
    "print(encoding_result)\n",
    "print(encoding_test.decode(encoding_result))\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现了一个简单的函数来测试你编码的列表是否被正确解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode(l):\n",
    "    e = CompressedPostings.encode(l)\n",
    "    d = CompressedPostings.decode(e)\n",
    "    assert d == l\n",
    "    print(len(e))\n",
    "    print(l, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一些测试样例来确保文档列表的压缩和解压正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0, 1, 2, 3] b'\\x01\\x02\\x03\\x04'\n",
      "3\n",
      "[1, 2, 90] b'\\x02\\x03['\n",
      "7\n",
      "[23, 45, 90, 101, 1000000] b'\\x18.[fA\\x84\\xbd'\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "test_encode_decode([0, 1, 2, 3])\n",
    "test_encode_decode([1, 2, 90])\n",
    "test_encode_decode([23, 45, 90, 101, 1000000])\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们创建一个新的输出文件夹并使用`CompressedPostings`来构建压缩索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir_compressed')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0块时间为:  29.527858018875122\n",
      "第1块时间为:  27.996601343154907\n",
      "第2块时间为:  36.111323833465576\n",
      "第3块时间为:  28.676368474960327\n",
      "第4块时间为:  30.03512406349182\n",
      "第5块时间为:  29.272217273712158\n",
      "第6块时间为:  29.025397300720215\n",
      "第7块时间为:  23.958435535430908\n",
      "第8块时间为:  23.351813316345215\n",
      "第9块时间为:  22.72871994972229\n",
      "索引构建时间： 281.1043336391449\n",
      "索引合并时间： 1507.9019174575806\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\\\cs276.stanford.edu_',\n",
       " '1\\\\cs276a.stanford.edu_',\n",
       " '3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " '6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " '8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像之前一样，用已构造好的查询语句来测试是否正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match for query: we are\n",
      "Results match for query: stanford class\n",
      "Results match for query: stanford students\n",
      "Results match for query: very cool\n",
      "Results match for query: the\n",
      "Results match for query: a\n",
      "Results match for query: the the\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance_compressed.retrieve(query)]\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 额外的编码方式 (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过补充`ECCompressedPostings`的`encode` 和 `decode`方法来实现一种额外的索引压缩方式。在我们课上学的就是**gamma-encoding** 。另外如果大家感兴趣的话也可以了解**Delta Encoding** ，[ALGORITHM SIMPLE-9](https://github.com/manning/CompressionAlgorithms#simple-9) 等。\n",
    "\n",
    "你应该以多字节（而不是bits）来存储倒排记录表，因为索引的长度和位置都存的是字节信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECCompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    # 将单个整数进行编码的函数\n",
    "    @staticmethod\n",
    "    def do_number_encoding(num):\n",
    "        num += 2  # 为了处理0，所有数都+2\n",
    "        length = math.floor(math.log2(num)) + 1  # 位数\n",
    "        later = 2 ** (length - 1) - 1  # 一元编码\n",
    "        former = num & later  # 原二进制除去最高位后的内容\n",
    "        after_encoding = (former << length ) | later  # 为方便解码，将原二进制的剩余内容放在高位，将一元编码放在低位\n",
    "        return length * 2 - 1, after_encoding\n",
    "    \n",
    "    @staticmethod\n",
    "    def do_number_decoding(num):\n",
    "        decoded_postings_list = []\n",
    "        bit = 8\n",
    "        bit_num = 256\n",
    "        # 当解码未结束时\n",
    "        while num > 0:\n",
    "            this_length = 0  # 当前整数转换为二进制后的长度-1\n",
    "            reminder = num % bit_num  # 为加快解码速度，1次取8位\n",
    "            # 当前位依然是一元编码的内容\n",
    "            while 1:\n",
    "                if reminder == big_num - 1:\n",
    "                    this_length += bit\n",
    "                    num = num >> bit\n",
    "                    reminder = num % bit_num\n",
    "                    continue\n",
    "                # 当前4位中有0出现\n",
    "                while 1:\n",
    "                    # 每两位找0\n",
    "                    if reminder % 4 == 3:\n",
    "                        reminder = reminder >> 2\n",
    "                        num = num >> 2\n",
    "                        this_length += 2\n",
    "                        continue\n",
    "                    # 1位1位找1\n",
    "                    while reminder % 2 == 1:\n",
    "                        reminder = reminder >> 1\n",
    "                        num = num >> 1\n",
    "                        this_length += 1\n",
    "                    num = num >> 1\n",
    "                    break\n",
    "                break\n",
    "            biggest_number = 2 ** this_length\n",
    "            this_num = (num % (biggest_number)) + biggest_number  # 当前正在解码的整数\n",
    "            decoded_postings_list.append(this_num - 2)\n",
    "            num = num >> this_length\n",
    "        return decoded_postings_list\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        result = 0  # 结果\n",
    "        all_len = 0  # 编码后的总长度\n",
    "        for docId in postings_list:\n",
    "            length, binary = ECCompressedPostings.do_number_encoding(docId)  # 获得当前数字编码后的长度和内容\n",
    "            all_len += length\n",
    "            result = (result << length) | binary  # 加入结果编码\n",
    "        return result.to_bytes(math.ceil(all_len / 8), byteorder='big')\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docId)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        num = int.from_bytes(encoded_postings_list, byteorder='big')\n",
    "        decoded_postings_list = ECCompressedPostings.do_number_decoding(num)\n",
    "        decoded_postings_list.reverse()\n",
    "        return decoded_postings_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4k'\n",
      "[3, 2, 1, 0]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "encoding_test = ECCompressedPostings()\n",
    "encoding_result = encoding_test.encode([0, 1, 2, 3])\n",
    "print(encoding_result)\n",
    "print(encoding_test.do_number_decoding(int.from_bytes(encoding_result, byteorder='big')))\n",
    "print(encoding_test.decode(encoding_result))\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode_gamma(l):\n",
    "    e = ECCompressedPostings.encode(l)\n",
    "    d = ECCompressedPostings.decode(e)\n",
    "    assert d == l\n",
    "    print(len(e))  # 输出编码后的长度\n",
    "    print(l, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0, 1, 2, 3] b'4k'\n",
      "3\n",
      "[1, 2, 90] b'\\x14n?'\n",
      "11\n",
      "[23, 45, 90, 101, 1000000] b\"\\x12\\xf7\\xbe\\xe3\\xf9\\xdf\\xf4$'\\xff\\xff\"\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "test_encode_decode_gamma([0, 1, 2, 3])\n",
    "test_encode_decode_gamma([1, 2, 90])\n",
    "test_encode_decode_gamma([23, 45, 90, 101, 1000000])\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建新目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir_ECcompressed')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0块时间为:  31.68158531188965\n",
      "第1块时间为:  29.765013694763184\n",
      "第2块时间为:  32.603771686553955\n",
      "第3块时间为:  27.59824013710022\n",
      "第4块时间为:  29.69919466972351\n",
      "第5块时间为:  28.191696166992188\n",
      "第6块时间为:  26.90467119216919\n",
      "第7块时间为:  21.81041646003723\n",
      "第8块时间为:  22.604939222335815\n",
      "第9块时间为:  21.553518533706665\n",
      "索引构建时间： 272.8205726146698\n",
      "索引合并时间： 1404.7573945522308\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance_ECcompressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_ECcompressed', postings_encoding=ECCompressedPostings)\n",
    "BSBI_instance_ECcompressed.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\\\cs276.stanford.edu_',\n",
       " '1\\\\cs276a.stanford.edu_',\n",
       " '3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " '6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " '8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance_ECcompressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_ECcompressed', postings_encoding=ECCompressedPostings)\n",
    "BSBI_instance_ECcompressed.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.33920383453369\n",
      "Results match for query: we are\n",
      "108.35786318778992\n",
      "Results match for query: stanford class\n",
      "122.47082448005676\n",
      "Results match for query: stanford students\n",
      "0.6513309478759766\n",
      "Results match for query: very cool\n",
      "143.02038073539734\n",
      "Results match for query: the\n",
      "94.63334465026855\n",
      "Results match for query: a\n",
      "145.02497911453247\n",
      "Results match for query: the the\n",
      "114.05165648460388\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    start_time = time.time()\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance_ECcompressed.retrieve(query)]\n",
    "        print(time.time() - start_time)  # 输出查询的运行时间\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业提交\n",
    "\n",
    "本次作业用时两周，截止日期为11.4。请大家在截止日期前将代码（包含运行结果，测试内容不作要求），实验报告（可单独撰写，也可整合在jupyter notebook中）一起提交到×××，命名方式为`学号_姓名_hw4`。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
